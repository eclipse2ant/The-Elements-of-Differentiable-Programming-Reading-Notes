{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxG8O5T9BzuLe/j9JLzFXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eclipse2ant/The-Elements-of-Differentiable-Programming-Reading-Notes/blob/master/note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://arxiv.org/abs/2403.14606\n",
        "iいくつか疑問点をめもっておく\n",
        "思いつきだけど、ひも理論のコンパクト化と関連付けられないだろうか？エネルギーの低い今の状態は、ブレーンやフラックスが巻き付いてるおかげで成り立ってるとしたら、ヤウの見えざる宇宙の形を読みながら、３体の十次元の話とも関連するけど、真面目に物理やさんは、デコンバクト化の可能性を考えてる\n",
        "量子化したらトンネル効果で、相転移が、起こるとしたら、機械学習は、どうなるんだろう？素朴な疑問？\n",
        "・関数は純粋関数でないといけないけど、確率論の話が出てくるけどランダム性が入ると純粋関数でなくなるから、どうかかわってるのか？　マルコフ連鎖とかベイジアン・ネットワークとか\n",
        "・https://github.com/taichi-dev/difftaichi　との関係？\n",
        "https://wirelesswire.jp/2021/07/80265/...\n",
        "の話で\n",
        "ディープラーニングにおいては、この「微分可能」という性質を最大限に用いる。\n",
        "この計算プロセスは、メモリと計算能力の許す限り、いくらでも連鎖させることが可能で、かつ、逆戻りできる。\n",
        "たとえば、画像を入力して、途中にランダムな数値パラメータによるネットワークを置き、最後に出力された数値と、本来欲しかった数値の差分を取り、逆伝搬させて諸々の数値を調整する、と言うのがディープラーニングの根本的な原理である。\n",
        "元々、微分可能プログラミングというのは、ディープラーニングの副産物であった。したがって、これまでは微分可能プログラミングの適用範囲は、ニューラルネットワークの中だけに制限されているのが普通だった。\n",
        "しかしここ数年で、ニューラルネットという範囲を超えた、「微分可能な○○」が次々に登場している。\n",
        "とあるけど、逆伝搬で個々の数値を調整するというからくりは、統一的な記述はないのかなぁ？　思いつきでは　物理の逆散乱法とか（言葉しかしらないから間違ってるかも）　フェルマー予想の証明にでてくるワイルズのオイラーシステムとかつかえないのかなぁ？これも全然全然勉強不足で知らないけど、高次の項を低次の項だけの情報から芋ずる式で決めていく話のような感じに思ってるけど、言葉が難しくてまだ勉強してない。\n",
        "・微分を使うと次元の呪いが解消すると書いてるけどなんで？？これって、正則化のこと？？\n",
        "ーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n",
        "いくつかわかったこと\n",
        "関数型プログラミングを意識すると、関数（数学ではスカラー値でない限りはあまり使わない用語、普通は写像という以下では　ベクトル値関数とします）は簡単なベクトル値関数の合成で表せます。ただし　合成したものは　スカラー値なので、スカラー値関数になってます（もちろん多変数ですね）\n",
        "事実としては　　grad f は　df　で各変数でf を偏微分したものを並べたもの　今は横ベクトルで考えます。\n",
        "一般に　ベクトル値関数の　ヤコビ行列は dF と　表します。　だから　スカラー値の時は grad f に一致します。\n",
        "ｆをベクトル値関数の合成として表せるので　f=g◦F_n◦・・・・◦F_1 　表します。g はスカラー値関数です。\n",
        "チェーンルールって実は　合成は　それぞれの　ヤコビ行列の積で書けるということです。\n",
        "df=dg◦dF_n◦・・・・◦dF_1    こちらは　行列の積です。\n",
        "ここで　autodiff の話で、　　例えば　　vCBA　（ｖは横ベクトル）　を　右側から順番に行列の積をとって計算すると行列サイズが大きいとそれなりにメモリー食いますよね\n",
        "こっちがFowward-mode\n",
        "左側から　計算したら　横ベクトルが出てきますから、サイズが小さくて済みます。同じことを　df の転置をとってやります。そうすると　転置は積の順番を入れ替えますから\n",
        "転置をとってから右側から計算したら、左から計算したことと同じになります。これが論文についてる*の意味です。\n",
        "こちらがReverse-mode\n",
        "たぶんこれで解釈があっていると思います。\n",
        "まあ、簡単にいうと f=g◦F 　として　df = dg◦ｄF = (dF)^* (dg)\n",
        "(dF)^* は dF の双対写像　だから表現行列は元の表現行列の転置だから積の順番が入れ替わる。\n",
        "多様体論つかうと　接束と余接束と写像の微分とその双対ってことで\n",
        "dg　って微分形式で考えて、そうすると　F:M-->N なら\n",
        "ｄF: T(M)--> T(N) で　(dF)^* : T^*(N) --> T^*(M) で　dg はT^*(N)\n",
        "のsection（断面）ってことなんですが。。。"
      ],
      "metadata": {
        "id": "dpczpXFCih1m"
      }
    }
  ]
}