{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjEAdbEEByNI/YvZwxOHMO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eclipse2ant/The-Elements-of-Differentiable-Programming-Reading-Notes/blob/master/note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2403.14606"
      ],
      "metadata": {
        "id": "j-mW1xWBNlUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# いくつか疑問点をめもっておく"
      ],
      "metadata": {
        "id": "BGCFR_RTNzb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 思いつきだけど、ひも理論のコンパクト化と関連付けられないだろうか？エネルギーの低い今の状態は、ブレーンやフラックスが巻き付いてるおかげで成り立ってるとしたら、ヤウの見えざる宇宙の形を読みながら、３体の十次元の話とも関連するけど、真面目に物理やさんは、デコンバクト化の可能性を考えてる。量子化したらトンネル効果で、相転移が、起こるとしたら、機械学習は、どうなるんだろう？素朴な疑問？"
      ],
      "metadata": {
        "id": "ZQbwpdQpOEsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 関数は純粋関数でないといけないけど、確率論の話が出てくるけどランダム性が入ると純粋関数でなくなるから、どうかかわってるのか？　マルコフ連鎖とかベイジアン・ネットワークとか"
      ],
      "metadata": {
        "id": "Ulbl4rePOS-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://github.com/taichi-dev/difftaichi　との関係？<br> https://wirelesswire.jp/2021/07/80265/... の話で ディープラーニングにおいては、この「微分可能」という性質を最大限に用いる。 この計算プロセスは、メモリと計算能力の許す限り、いくらでも連鎖させることが可能で、かつ、逆戻りできる。 たとえば、画像を入力して、途中にランダムな数値パラメータによるネットワークを置き、最後に出力された数値と、本来欲しかった数値の差分を取り、逆伝搬させて諸々の数値を調整する、と言うのがディープラーニングの根本的な原理である。 元々、微分可能プログラミングというのは、ディープラーニングの副産物であった。したがって、これまでは微分可能プログラミングの適用範囲は、ニューラルネットワークの中だけに制限されているのが普通だった。 しかしここ数年で、ニューラルネットという範囲を超えた、「微分可能な○○」が次々に登場している。 とあるけど、逆伝搬で個々の数値を調整するというからくりは、統一的な記述はないのかなぁ？　思いつきでは　物理の逆散乱法とか（言葉しかしらないから間違ってるかも）　フェルマー予想の証明にでてくるワイルズのオイラーシステムとかつかえないのかなぁ？これも全然全然勉強不足で知らないけど、高次の項を低次の項だけの情報から芋ずる式で決めていく話のような感じに思ってるけど、言葉が難しくてまだ勉強してない。"
      ],
      "metadata": {
        "id": "e3qQw08VOmnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "・[DiffTaichi: Differentiable Programming for Physical Simulation (ICLR 2020)](https://github.com/taichi-dev/difftaichi)　との関係？<br>"
      ],
      "metadata": {
        "id": "MYZGBWMzP53G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[映像も物理も、微分可能になるとすごいことが起きる」ということの意味を文系にもわかるように説明しようと試みる](https://wirelesswire.jp/2021/07/80265/?fbclid=IwZXh0bgNhZW0CMTAAAR3G0io07wiH7_LInQJxEb805m0s1QlHOizkCGhjQMR9NNPamDwDJM9Tr_c_aem_wnEjffF__OGrmcbx6tM6Qw)\n",
        "の話でディープラーニングにおいては、この「微分可能」という性質を最大限に用いる。\n"
      ],
      "metadata": {
        "id": "tk2t8eH4QIof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "この計算プロセスは、メモリと計算能力の許す限り、いくらでも連鎖させることが可能で、かつ、逆戻りできる。\n",
        "たとえば、画像を入力して、途中にランダムな数値パラメータによるネットワークを置き、最後に出力された数値と、本来欲しかった数値の差分を取り、逆伝搬させて諸々の数値を調整する、と言うのがディープラーニングの根本的な原理である。\n",
        "元々、微分可能プログラミングというのは、ディープラーニングの副産物であった。したがって、これまでは微分可能プログラミングの適用範囲は、ニューラルネットワークの中だけに制限されているのが普通だった。\n",
        "しかしここ数年で、ニューラルネットという範囲を超えた、「微分可能な○○」が次々に登場している。\n",
        "とあるけど、逆伝搬で個々の数値を調整するというからくりは、統一的な記述はないのかなぁ？　思いつきでは　物理の逆散乱法とか（言葉しかしらないから間違ってるかも）　フェルマー予想の証明にでてくるワイルズのオイラーシステムとかつかえないのかなぁ？これも全然全然勉強不足で知らないけど、高次の項を低次の項だけの情報から芋ずる式で決めていく話のような感じに思ってるけど、言葉が難しくてまだ勉強してない。"
      ],
      "metadata": {
        "id": "Z0vVCIsvQO9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 微分を使うと次元の呪いが解消すると書いてるけどなんで？？これって、正則化のこと？？"
      ],
      "metadata": {
        "id": "yzLiTKpMQR4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ーーーーーーーーーーーーーーーーーーーーーーーーーーーーー<br>\n"
      ],
      "metadata": {
        "id": "GutLe7BYVL4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# いくつかわかったこと"
      ],
      "metadata": {
        "id": "BsDIaBAvVQ5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autodiss について"
      ],
      "metadata": {
        "id": "3m_73CL58V9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "関数型プログラミングを意識すると、関数（数学ではスカラー値でない限りはあまり使わない用語、普通は写像という以下では　ベクトル値関数とします）は簡単なベクトル値関数の合成で表せます。ただし　合成したものは　スカラー値なので、スカラー値関数になってます（もちろん多変数ですね）<br>\n",
        "事実としては　$grad\\ f$ は $df$ で各変数でf を偏微分したものを並べたもの　今は横ベクトルで考えます。<br>\n",
        "一般に　ベクトル値関数の　ヤコビ行列は $dF$ と表します。だからスカラー値の時は $grad f$ に一致します。<br>\n",
        "$f$ をベクトル値関数の合成として表せるので $f=g\\circ F_n\\circ\\cdots \\circ F_1$ と表します。$g$ はスカラー値関数です。<br>\n",
        "チェーンルールって実は合成はそれぞれのヤコビ行列の積で書けるということです。<br>\n",
        "$df=dg \\circ dF_n \\circ \\cdots \\circ dF_1$    こちらは　行列の積です。\n",
        "ここで autodiff の話で、例えば $vCBA$（$v$は横ベクトル）を右側から順番に行列の積をとって計算すると行列サイズが大きいとそれなりにメモリー食いますよね\n",
        "こっちがFowward-mode<br>\n",
        "左側から計算したら横ベクトルが出てきますから、サイズが小さくて済みます。同じことを $df$ の転置をとってやります。そうすると転置は積の順番を入れ替えますから\n",
        "転置をとってから右側から計算したら、左から計算したことと同じになります。これが論文についてる*の意味です。\n",
        "こちらがReverse-mode<br>\n",
        "たぶんこれで解釈があっていると思います。<br>\n"
      ],
      "metadata": {
        "id": "dpczpXFCih1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まあ、簡単にいうと $f=g◦F$ として $df = dg◦dF = (dF)^* (dg)$<br>\n",
        "ここで $(dF)^*$ は $dF$ の双対写像だから表現行列は元の表現行列の転置だから積の順番が入れ替わる。<br>\n",
        "多様体論つかうと　接束と余接束と写像の微分とその双対ってことで\n",
        "$dg$ って微分形式で考えて、そうすると $F:M \\longrightarrow N$ なら\n",
        "$dF: T(M) \\longrightarrow T(N)$ で $(dF)^* : T^*(N) \\longrightarrow T^*(M)$ で $dg$ は $T^*(N)$ のsection（断面）ってことなんですが。。。"
      ],
      "metadata": {
        "id": "jP_tXNcUTbqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 何を微分するか？"
      ],
      "metadata": {
        "id": "9wW_L67m8pcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "アイシャさん曰く、機械学習は関数近似ということで $F$ をベクトル値関数としたら $F_w$でなるべく $F$を近似しろということになります。<br>\n",
        "$F, F_w : U \\longrightarrow V$ をベクトル値関数とすると $|| F-F_w || \\longrightarrow 0$ になるように $W$ の元 $w$を見つけなさいということで、このノルム $||\\ ||$ での評価は前述の関数 $g$ に置き替えられます。このままだと微分するのは大変です。<br>\n",
        "$Hom(W, Hom(U,V)) = Hom(W\\times U, V)$ 本当は等号でなく同型ですが、同一視してますが、パラーメータ付き関数は、２変数の関数と思えるわけです。いわゆるカーリー化です。圏論の言葉で $(\\cdot)\\times U$ と $Hom(U,\\ \\cdot\\ )$ は随伴関手ということです。<br>\n",
        "かつ $U$ の値をいくつか決めて固定してしまえば、$W$ から $V$ へのベクトル値関数になり微分が使えます。<br>\n",
        "if や else 分岐などのコントロール、フローなども関数と見て微分可能にします。このあたりは smoothing のはなし、\n"
      ],
      "metadata": {
        "id": "fc-KZtmv0xg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MCMC や　ベイズ推定　との関連"
      ],
      "metadata": {
        "id": "0ikQsXl18wCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "もう一つ MCMC とか ベイス推定で、出てくるマルコフ連鎖ですが、知りたい確率の確率分布（これはパラメータライズされていて $w$ で与えられるとします）で決まる遷移確率の行列 $A_w$ を事前確率の確率分布（これはヒストグラムで与えられるので有限個の取りうる状態に対する確率のヒストグラムで、それぞれの状態をとりうる確率が縦ベクトルの成分の値）をあたえるベクトル $v$ に施します。<br>\n",
        "$A_w$ は $w$ のほかにベイスの定理で出てくる測定したデータの結果にも依存します。ここは先ほどのUと一緒でデーターの結果で固定します。<br>\n",
        "それで $A_w$ の $n$ 乗を考えます。 $A_w$ はペロン・フロベニウスの定理で固有値が $1$がひとつだけあとはすべて $1$ より真に小さいとします。そうすると $n$ を無限大に飛ばすと固有値は $1$ だけ生きのこり、後は$0$ になります。ということは $A_w$ を無限回 $v$ に施せば、固有値 $1$ に対する固有ベクトルが自然にあらわれます。これが求める事後確率の確率分布です。この最大値を求めてあげれば $W$ の関数になって $W$に関して微分して最大値を求めるのが最大尤度推定です。これも乱数とかシミューレーションで使う値を固定してしまえば純粋関数として扱えて、自動微分が使えるというわけです。<br>\n"
      ],
      "metadata": {
        "id": "TyafBKtQ3i6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 正則化"
      ],
      "metadata": {
        "id": "VVt_FI_X837q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "論文の最後に、次元の呪いを解消するための正則化 これは $L^1,\\ L^2,$ 双対の三つありますが、これらとの関係はまだよくわかりません。あと自動微分はヘシアンにも有効で、これは ヘシアン自体が $df$ の各成分関数をさらに $d$ をとる操作なので、それぞれの成分をまた自動微分すればよいわけです。ヘシアン自体はその行列が正定値ならまあ曲率が正ということで、ちょうど高校数学の $2$次導関数と同じ使い方です。それの多変数版です。\n",
        "正則化については\n",
        "https://www.jstage.jst.go.jp/article/bjsiam/28/2/28_28/_pdf"
      ],
      "metadata": {
        "id": "42nC3NXJ7cYa"
      }
    }
  ]
}